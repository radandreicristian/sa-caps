# id-sf

Learning the Relationship Between Intents and Slots via Knowledge Distillation.
Bib:

1) CapsNets
Hinton, 2012 - Capsule Networks (tbd)
Sabour, 2015 - Dynamic Routing (tbd)
Zhang, 2019 - CapsNet ID-SF (tbd)

2) Transformers
Vaswani, 2016 - Attention (for encoder)
Devlin, 2017 - Bert (for _CLS_, masking, etc)
Bunk, 2020 - DIET (sparse features, nice transformataions)

3) TransCaps stuff
Mobiny, 2020 - Trans-Caps (vision)
Duan, 2020 - Capsule-Transformer for Translation
Obuchowski, 2020 - Transformer CapsNet for ID

4) More ID-SF (: :)
Haihong, 2019 - Interrelated Model (maybe as teacher?)

5) Knowledge Distillation
Hinton, 2015 - Distilling the knowledge in a neural network
Touvron, 2021 - DeiT (distillation through attention)

6) Others
Henderson, 2020 - ConveRT Embeddings

Objectives / Ideas:

1. Knowledge distillation through attention, whys and why-nots

